{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedb77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da99f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sentence\n",
    "sent_short = pd.read_csv('data/sentence_short.csv')\n",
    "sent_short_sim = pd.read_csv('data/sentence_short_simple.csv')\n",
    "sent_short_past = pd.read_csv('data/sentence_short_past.csv')\n",
    "sent_short_ps = pd.read_csv('data/sentence_short_ps.csv')\n",
    "sent_long = pd.read_csv('data/sentence_long.csv')\n",
    "sent_long_sim = pd.read_csv('data/sentence_long_simple.csv')\n",
    "sent_long_past = pd.read_csv('data/sentence_long_past.csv')\n",
    "sent_long_ps = pd.read_csv('data/sentence_long_ps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a4fe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# disable gradient computations\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# load the BERT model\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7b2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax function that we'll use later on\n",
    "f_softmax = torch.nn.Softmax(dim = 0)\n",
    "\n",
    "# determine whether give the correct token the higher possibility\n",
    "def computeCorrectness(df):\n",
    "    \n",
    "    df_new = df.copy()\n",
    "    prediction_list = []\n",
    "    accuracy_list = []\n",
    "    correct_probs = []\n",
    "    wrong_probs = []\n",
    "    \n",
    "    for i in range(0, len(df_new)):\n",
    "        sentence = df_new['sentence'][i]\n",
    "        correct = df_new['correct'][i]\n",
    "        wrong = df_new['wrong'][i]\n",
    "        \n",
    "        # convert words to indices\n",
    "        inputs = tokenizer(sentence, return_tensors = \"pt\")\n",
    "        \n",
    "        # get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # determine position of the masked token\n",
    "        mask_index = int((inputs['input_ids'][0] == tokenizer.mask_token_id).nonzero())\n",
    "        \n",
    "        # determine correct and wrong indices\n",
    "        correct_index = tokenizer.convert_tokens_to_ids(correct)\n",
    "        wrong_index = tokenizer.convert_tokens_to_ids(wrong)\n",
    "        \n",
    "        # convert logits to probabilities with softmax\n",
    "        probs = f_softmax(outputs['logits'][0, mask_index, :])\n",
    "        \n",
    "        # extract logits and probability for correct and wrong tokens\n",
    "        correct_logit = outputs.logits[0, mask_index, correct_index]\n",
    "        correct_probs.append(probs[correct_index])\n",
    "        wrong_logit = outputs.logits[0, mask_index, wrong_index]\n",
    "        wrong_probs.append(probs[wrong_index])\n",
    "        \n",
    "        # correct prediction insert correct, wrong prediction insert wrong\n",
    "        if correct_logit > wrong_logit:\n",
    "            prediction_list.append('correct')\n",
    "            accuracy_list.append(1)\n",
    "        elif wrong_logit > correct_logit:\n",
    "            prediction_list.append('wrong')\n",
    "            accuracy_list.append(0)\n",
    "        else:\n",
    "            # no clear preference\n",
    "            prediction_list.append('no preference')\n",
    "            accuracy_list.append(0)\n",
    "    \n",
    "    df_new['prediction'] = prediction_list\n",
    "    df_new['accuracy'] = accuracy_list\n",
    "    df_new['correct_prob'] = correct_probs\n",
    "    df_new['wrong_prob'] = wrong_probs\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692f1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax function that we'll use later on\n",
    "f_softmax = torch.nn.Softmax(dim = 0)\n",
    "\n",
    "# determine if the top prediction is our correct token,\n",
    "# and of our correct token is in top five prediction\n",
    "def getTopPredictions(df):\n",
    "    \n",
    "    df_new = df.copy()\n",
    "    top_list = []\n",
    "    top5_list = []\n",
    "    top5_prob = []\n",
    "    top_is_correct = []\n",
    "    top_is_wrong = []\n",
    "    correct_in_top5 = []\n",
    "    wrong_in_top5 = []\n",
    "    \n",
    "    for i in range(0, len(df_new)):\n",
    "        sentence = df_new['sentence'][i]\n",
    "        correct = df_new['correct'][i]\n",
    "        wrong = df_new['wrong'][i]\n",
    "        \n",
    "        # convert words to indices\n",
    "        inputs = tokenizer(sentence, return_tensors = \"pt\")\n",
    "        \n",
    "        # get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # determine position of the masked token\n",
    "        mask_index = int((inputs['input_ids'][0] == tokenizer.mask_token_id).nonzero())\n",
    "        \n",
    "        # convert logits to probabilities with softmax\n",
    "        probs = f_softmax(outputs['logits'][0, mask_index, :])\n",
    "        \n",
    "        # select top 5 predictions\n",
    "        top_prediction_ids = torch.argsort(probs, descending = True)[0:5]\n",
    "        top_prediction_tokens = [tokenizer.convert_ids_to_tokens(int(i)) \n",
    "                                 for i in top_prediction_ids]\n",
    "        top_list.append(top_prediction_tokens[0])\n",
    "        top5_list.append(top_prediction_tokens)\n",
    "        top5_prob.append([probs[i] for i in top_prediction_ids])\n",
    "        \n",
    "        # determine whether top prediction is correct/wrong, or whether it is in top 5\n",
    "        if correct == top_prediction_tokens[0]:\n",
    "            top_is_correct.append(1)\n",
    "        else:\n",
    "            top_is_correct.append(0)\n",
    "        if wrong == top_prediction_tokens[0]:\n",
    "            top_is_wrong.append(1)\n",
    "        else:\n",
    "            top_is_wrong.append(0)\n",
    "            \n",
    "        # determine whether correct/wrong token is in the top five prediction\n",
    "        if correct in top_prediction_tokens:\n",
    "            correct_in_top5.append(1)\n",
    "        else:\n",
    "            correct_in_top5.append(0)\n",
    "        if wrong in top_prediction_tokens:\n",
    "            wrong_in_top5.append(1)\n",
    "        else:\n",
    "            wrong_in_top5.append(0)\n",
    "    \n",
    "    df_new['top predict'] = top_list\n",
    "    df_new['top 5 predict'] = top5_list\n",
    "    df_new['top 5 prob'] = top5_prob\n",
    "    df_new['top is correct'] = top_is_correct\n",
    "    df_new['top is wrong'] = top_is_wrong\n",
    "    df_new['correct in top 5'] = correct_in_top5\n",
    "    df_new['wrong in top 5'] = wrong_in_top5\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d21d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the result dataframes\n",
    "sent_short_result = getTopPredictions(computeCorrectness(sent_short))\n",
    "sent_short_sim_result = getTopPredictions(computeCorrectness(sent_short_sim))\n",
    "sent_short_past_result = getTopPredictions(computeCorrectness(sent_short_past))\n",
    "sent_short_ps_result = getTopPredictions(computeCorrectness(sent_short_ps))\n",
    "sent_long_result = getTopPredictions(computeCorrectness(sent_long))\n",
    "sent_long_sim_result = getTopPredictions(computeCorrectness(sent_long_sim))\n",
    "sent_long_past_result = getTopPredictions(computeCorrectness(sent_long_past))\n",
    "sent_long_ps_result = getTopPredictions(computeCorrectness(sent_long_ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the results\n",
    "# sent_short_result.to_csv('result/sent_short_result.csv', index = False)\n",
    "# sent_short_sim_result.to_csv('result/sent_short_sim_result.csv', index = False)\n",
    "# sent_short_past_result.to_csv('result/sent_short_past_result.csv', index = False)\n",
    "# sent_short_ps_result.to_csv('result/sent_short_ps_result.csv', index = False)\n",
    "# sent_long_result.to_csv('result/sent_long_result.csv', index = False)\n",
    "# sent_long_sim_result.to_csv('result/sent_long_sim_result.csv', index = False)\n",
    "# sent_long_past_result.to_csv('result/sent_long_past_result.csv', index = False)\n",
    "# sent_long_ps_result.to_csv('result/sent_long_ps_result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
