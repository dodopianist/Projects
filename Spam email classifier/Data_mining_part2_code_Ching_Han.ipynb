{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2510afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901a5e8",
   "metadata": {},
   "source": [
    "# 0. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e92678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f629997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('part2-data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af66b3",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc23deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1932ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43350245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    text_list = [text.lower() for text in df['text']]\n",
    "    stop_words = [item for item in ENGLISH_STOP_WORDS]\n",
    "    punctuations = [item for item in string.punctuation]\n",
    "    \n",
    "    subject = [re.findall(f'subject: (.*)\\r', text)[0] for text in text_list]\n",
    "    df['subject'] = subject\n",
    "    \n",
    "    subject_word = [re.findall(f'subject: (.*)\\r', text)[0].split(' ') for text in text_list]\n",
    "    df['subject_word'] = subject_word\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    subject_lemma = []\n",
    "    subject_lemma_concat = []\n",
    "    for item in subject_word:\n",
    "        subject_tagged = pos_tag(item)\n",
    "        subject_list = [\n",
    "            lemmatizer.lemmatize(word, pos = get_wordnet_pos(pos)) for word, pos in subject_tagged \n",
    "            if not word in stop_words + punctuations\n",
    "        ]\n",
    "        subject_lemma.append(subject_list)\n",
    "        subject_lemma_concat.append(' '.join(subject_list))\n",
    "    df['subject_lemma'] = subject_lemma\n",
    "    df['subject_lemma_concat'] = subject_lemma_concat\n",
    "    \n",
    "    content = [re.sub(f'subject: .*\\r\\n', '', text).replace('\\r\\n', ' ') for text in text_list]\n",
    "    df['content'] = content\n",
    "    \n",
    "    content_word = [re.sub(f'subject: .*\\r\\n', '', text).replace('\\r\\n', ' ').split(' ') for text in text_list]\n",
    "    df['content_word'] = content_word\n",
    "    \n",
    "    content_lemma = []\n",
    "    content_lemma_concat = []\n",
    "    for item in content_word:\n",
    "        content_tagged = pos_tag(item)\n",
    "        content_list = [\n",
    "            lemmatizer.lemmatize(word, pos = get_wordnet_pos(pos)) for word, pos in content_tagged \n",
    "            if not word in stop_words + punctuations\n",
    "        ]\n",
    "        content_lemma.append(content_list)\n",
    "        content_lemma_concat.append(' '.join(content_list))\n",
    "    df['content_lemma'] = content_lemma\n",
    "    df['content_lemma_concat'] = content_lemma_concat\n",
    "    \n",
    "    content_sent_raw = [re.sub(f'subject: .*\\r\\n', '', text).split('\\r\\n') for text in text_list]\n",
    "    content_sent = []\n",
    "    for item in content_sent_raw:\n",
    "        word = []\n",
    "        for sent in item:\n",
    "            word.append(sent.split(' '))\n",
    "        content_sent.append(word)\n",
    "    df['content_sent'] = content_sent\n",
    "    \n",
    "    full_text = [subject_word[i] + content_word[i] for i in range(0 ,len(df))]\n",
    "    df['full_text'] = full_text\n",
    "    \n",
    "    full_text_concat = [' '.join(df['full_text'][i]) for i in range(0 ,len(df))]\n",
    "    df['full_text_concat'] = full_text_concat\n",
    "    \n",
    "    full_lemma = [subject_lemma[i] + content_lemma[i] for i in range(0 ,len(df))]\n",
    "    df['full_lemma'] = full_lemma\n",
    "    \n",
    "    full_lemma_concat = [subject_lemma_concat[i] + ' ' + content_lemma_concat[i] for i in range(0 ,len(df))]\n",
    "    df['full_lemma_concat'] = full_lemma_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34575c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcfb5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2be8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_word</th>\n",
       "      <th>subject_lemma</th>\n",
       "      <th>subject_lemma_concat</th>\n",
       "      <th>content</th>\n",
       "      <th>content_word</th>\n",
       "      <th>content_lemma</th>\n",
       "      <th>content_lemma_concat</th>\n",
       "      <th>content_sent</th>\n",
       "      <th>full_text</th>\n",
       "      <th>full_text_concat</th>\n",
       "      <th>full_lemma</th>\n",
       "      <th>full_lemma_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>ham</td>\n",
       "      <td>enron methanol ; meter # : 988291</td>\n",
       "      <td>[enron, methanol, ;, meter, #, :, 988291]</td>\n",
       "      <td>[enron, methanol, meter, 988291]</td>\n",
       "      <td>enron methanol meter 988291</td>\n",
       "      <td>this is a follow up to the note i gave you on ...</td>\n",
       "      <td>[this, is, a, follow, up, to, the, note, i, ga...</td>\n",
       "      <td>[follow, note, give, monday, 4, 3, 00, prelimi...</td>\n",
       "      <td>follow note give monday 4 3 00 preliminary flo...</td>\n",
       "      <td>[[this, is, a, follow, up, to, the, note, i, g...</td>\n",
       "      <td>[enron, methanol, ;, meter, #, :, 988291, this...</td>\n",
       "      <td>enron methanol ; meter # : 988291 this is a fo...</td>\n",
       "      <td>[enron, methanol, meter, 988291, follow, note,...</td>\n",
       "      <td>enron methanol meter 988291 follow note give m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>ham</td>\n",
       "      <td>hpl nom for january 9 , 2001</td>\n",
       "      <td>[hpl, nom, for, january, 9, ,, 2001]</td>\n",
       "      <td>[hpl, nom, january, 9, 2001]</td>\n",
       "      <td>hpl nom january 9 2001</td>\n",
       "      <td>( see attached file : hplnol 09 . xls ) - hpln...</td>\n",
       "      <td>[(, see, attached, file, :, hplnol, 09, ., xls...</td>\n",
       "      <td>[attach, file, hplnol, 09, xl, hplnol, 09, xl]</td>\n",
       "      <td>attach file hplnol 09 xl hplnol 09 xl</td>\n",
       "      <td>[[(, see, attached, file, :, hplnol, 09, ., xl...</td>\n",
       "      <td>[hpl, nom, for, january, 9, ,, 2001, (, see, a...</td>\n",
       "      <td>hpl nom for january 9 , 2001 ( see attached fi...</td>\n",
       "      <td>[hpl, nom, january, 9, 2001, attach, file, hpl...</td>\n",
       "      <td>hpl nom january 9 2001 attach file hplnol 09 x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>ham</td>\n",
       "      <td>neon retreat</td>\n",
       "      <td>[neon, retreat]</td>\n",
       "      <td>[neon, retreat]</td>\n",
       "      <td>neon retreat</td>\n",
       "      <td>ho ho ho , we ' re around to that most wonderf...</td>\n",
       "      <td>[ho, ho, ho, ,, we, ', re, around, to, that, m...</td>\n",
       "      <td>[ho, ho, ho, wonderful, time, year, neon, lead...</td>\n",
       "      <td>ho ho ho wonderful time year neon leader retre...</td>\n",
       "      <td>[[ho, ho, ho, ,, we, ', re, around, to, that, ...</td>\n",
       "      <td>[neon, retreat, ho, ho, ho, ,, we, ', re, arou...</td>\n",
       "      <td>neon retreat ho ho ho , we ' re around to that...</td>\n",
       "      <td>[neon, retreat, ho, ho, ho, wonderful, time, y...</td>\n",
       "      <td>neon retreat ho ho ho wonderful time year neon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>photoshop , windows , office . cheap . main tr...</td>\n",
       "      <td>[photoshop, ,, windows, ,, office, ., cheap, ....</td>\n",
       "      <td>[photoshop, window, office, cheap, main, trend...</td>\n",
       "      <td>photoshop window office cheap main trending</td>\n",
       "      <td>abasements darer prudently fortuitous undergon...</td>\n",
       "      <td>[abasements, darer, prudently, fortuitous, und...</td>\n",
       "      <td>[abasement, darer, prudently, fortuitous, unde...</td>\n",
       "      <td>abasement darer prudently fortuitous undergone...</td>\n",
       "      <td>[[abasements, darer, prudently, fortuitous, un...</td>\n",
       "      <td>[photoshop, ,, windows, ,, office, ., cheap, ....</td>\n",
       "      <td>photoshop , windows , office . cheap . main tr...</td>\n",
       "      <td>[photoshop, window, office, cheap, main, trend...</td>\n",
       "      <td>photoshop window office cheap main trending ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>ham</td>\n",
       "      <td>re : indian springs</td>\n",
       "      <td>[re, :, indian, springs]</td>\n",
       "      <td>[indian, spring]</td>\n",
       "      <td>indian spring</td>\n",
       "      <td>this deal is to book the teco pvr revenue . it...</td>\n",
       "      <td>[this, deal, is, to, book, the, teco, pvr, rev...</td>\n",
       "      <td>[deal, book, teco, pvr, revenue, understanding...</td>\n",
       "      <td>deal book teco pvr revenue understanding teco ...</td>\n",
       "      <td>[[this, deal, is, to, book, the, teco, pvr, re...</td>\n",
       "      <td>[re, :, indian, springs, this, deal, is, to, b...</td>\n",
       "      <td>re : indian springs this deal is to book the t...</td>\n",
       "      <td>[indian, spring, deal, book, teco, pvr, revenu...</td>\n",
       "      <td>indian spring deal book teco pvr revenue under...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Subject: enron methanol ; meter # : 988291\\r\\n...   ham   \n",
       "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   ham   \n",
       "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   ham   \n",
       "3  Subject: photoshop , windows , office . cheap ...  spam   \n",
       "4  Subject: re : indian springs\\r\\nthis deal is t...   ham   \n",
       "\n",
       "                                             subject  \\\n",
       "0                  enron methanol ; meter # : 988291   \n",
       "1                       hpl nom for january 9 , 2001   \n",
       "2                                       neon retreat   \n",
       "3  photoshop , windows , office . cheap . main tr...   \n",
       "4                                re : indian springs   \n",
       "\n",
       "                                        subject_word  \\\n",
       "0          [enron, methanol, ;, meter, #, :, 988291]   \n",
       "1               [hpl, nom, for, january, 9, ,, 2001]   \n",
       "2                                    [neon, retreat]   \n",
       "3  [photoshop, ,, windows, ,, office, ., cheap, ....   \n",
       "4                           [re, :, indian, springs]   \n",
       "\n",
       "                                       subject_lemma  \\\n",
       "0                   [enron, methanol, meter, 988291]   \n",
       "1                       [hpl, nom, january, 9, 2001]   \n",
       "2                                    [neon, retreat]   \n",
       "3  [photoshop, window, office, cheap, main, trend...   \n",
       "4                                   [indian, spring]   \n",
       "\n",
       "                          subject_lemma_concat  \\\n",
       "0                  enron methanol meter 988291   \n",
       "1                       hpl nom january 9 2001   \n",
       "2                                 neon retreat   \n",
       "3  photoshop window office cheap main trending   \n",
       "4                                indian spring   \n",
       "\n",
       "                                             content  \\\n",
       "0  this is a follow up to the note i gave you on ...   \n",
       "1  ( see attached file : hplnol 09 . xls ) - hpln...   \n",
       "2  ho ho ho , we ' re around to that most wonderf...   \n",
       "3  abasements darer prudently fortuitous undergon...   \n",
       "4  this deal is to book the teco pvr revenue . it...   \n",
       "\n",
       "                                        content_word  \\\n",
       "0  [this, is, a, follow, up, to, the, note, i, ga...   \n",
       "1  [(, see, attached, file, :, hplnol, 09, ., xls...   \n",
       "2  [ho, ho, ho, ,, we, ', re, around, to, that, m...   \n",
       "3  [abasements, darer, prudently, fortuitous, und...   \n",
       "4  [this, deal, is, to, book, the, teco, pvr, rev...   \n",
       "\n",
       "                                       content_lemma  \\\n",
       "0  [follow, note, give, monday, 4, 3, 00, prelimi...   \n",
       "1     [attach, file, hplnol, 09, xl, hplnol, 09, xl]   \n",
       "2  [ho, ho, ho, wonderful, time, year, neon, lead...   \n",
       "3  [abasement, darer, prudently, fortuitous, unde...   \n",
       "4  [deal, book, teco, pvr, revenue, understanding...   \n",
       "\n",
       "                                content_lemma_concat  \\\n",
       "0  follow note give monday 4 3 00 preliminary flo...   \n",
       "1              attach file hplnol 09 xl hplnol 09 xl   \n",
       "2  ho ho ho wonderful time year neon leader retre...   \n",
       "3  abasement darer prudently fortuitous undergone...   \n",
       "4  deal book teco pvr revenue understanding teco ...   \n",
       "\n",
       "                                        content_sent  \\\n",
       "0  [[this, is, a, follow, up, to, the, note, i, g...   \n",
       "1  [[(, see, attached, file, :, hplnol, 09, ., xl...   \n",
       "2  [[ho, ho, ho, ,, we, ', re, around, to, that, ...   \n",
       "3  [[abasements, darer, prudently, fortuitous, un...   \n",
       "4  [[this, deal, is, to, book, the, teco, pvr, re...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  [enron, methanol, ;, meter, #, :, 988291, this...   \n",
       "1  [hpl, nom, for, january, 9, ,, 2001, (, see, a...   \n",
       "2  [neon, retreat, ho, ho, ho, ,, we, ', re, arou...   \n",
       "3  [photoshop, ,, windows, ,, office, ., cheap, ....   \n",
       "4  [re, :, indian, springs, this, deal, is, to, b...   \n",
       "\n",
       "                                    full_text_concat  \\\n",
       "0  enron methanol ; meter # : 988291 this is a fo...   \n",
       "1  hpl nom for january 9 , 2001 ( see attached fi...   \n",
       "2  neon retreat ho ho ho , we ' re around to that...   \n",
       "3  photoshop , windows , office . cheap . main tr...   \n",
       "4  re : indian springs this deal is to book the t...   \n",
       "\n",
       "                                          full_lemma  \\\n",
       "0  [enron, methanol, meter, 988291, follow, note,...   \n",
       "1  [hpl, nom, january, 9, 2001, attach, file, hpl...   \n",
       "2  [neon, retreat, ho, ho, ho, wonderful, time, y...   \n",
       "3  [photoshop, window, office, cheap, main, trend...   \n",
       "4  [indian, spring, deal, book, teco, pvr, revenu...   \n",
       "\n",
       "                                   full_lemma_concat  \n",
       "0  enron methanol meter 988291 follow note give m...  \n",
       "1  hpl nom january 9 2001 attach file hplnol 09 x...  \n",
       "2  neon retreat ho ho ho wonderful time year neon...  \n",
       "3  photoshop window office cheap main trending ab...  \n",
       "4  indian spring deal book teco pvr revenue under...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfe24f",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fa1f8",
   "metadata": {},
   "source": [
    "## 2-1. First Type of Feature: Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f6be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_count(df):\n",
    "    \n",
    "    length = pd.DataFrame()\n",
    "    \n",
    "    subject_c_count = []\n",
    "    subject_c_avg_w = []\n",
    "    subject_w_count = []\n",
    "    for item in df['subject_word']:\n",
    "        count = 0\n",
    "        for word in item:\n",
    "            count += len(word)\n",
    "        c_avg_w = count / len(item)\n",
    "        subject_c_count.append(count)\n",
    "        subject_c_avg_w.append(c_avg_w)\n",
    "        subject_w_count.append(len(item))\n",
    "    length['subject_c_count'] = subject_c_count\n",
    "    length['subject_c_avg_w'] = subject_c_avg_w\n",
    "    length['subject_w_count'] = subject_w_count\n",
    "\n",
    "    content_c_count = []\n",
    "    content_c_avg_w = []\n",
    "    content_w_count = []\n",
    "    for item in df['content_word']:\n",
    "        count = 0\n",
    "        for word in item:\n",
    "            count += len(word)\n",
    "        c_avg_w = count / len(item)\n",
    "        content_c_count.append(count)\n",
    "        content_c_avg_w.append(c_avg_w)\n",
    "        content_w_count.append(len(item))\n",
    "    length['content_c_count'] = content_c_count\n",
    "    length['content_c_avg_w'] = content_c_avg_w\n",
    "    length['content_w_count'] = content_w_count\n",
    "        \n",
    "    content_c_avg_s = []\n",
    "    content_w_avg_s = []\n",
    "    for i in range(0, len(df)):\n",
    "        count = 0\n",
    "        for word in df['content_word'][i]:\n",
    "            count += len(word)\n",
    "        count = count / len(df['content_sent'][i])\n",
    "        content_c_avg_s.append(count)\n",
    "        content_w_avg_s.append(len(df['content_word'][i]) / len(df['content_sent'][i]))\n",
    "    length['content_c_avg_s'] = content_c_avg_s\n",
    "    length['content_w_avg_s'] = content_w_avg_s\n",
    "        \n",
    "    content_s_count = [len(item) for item in df['content_sent']]\n",
    "    length['content_s_count'] = content_s_count\n",
    "    \n",
    "    subject_pun_count = []\n",
    "    subject_pun_w_pro = []\n",
    "    for item in df['subject_word']:\n",
    "        i = 0\n",
    "        for word in item:\n",
    "            if word in string.punctuation:\n",
    "                i += 1\n",
    "        subject_pun_count.append(i)\n",
    "        subject_pun_w_pro.append(i / len(item))\n",
    "    length['subject_pun_count'] = subject_pun_count\n",
    "    length['subject_pun_w_pro'] = subject_pun_w_pro\n",
    "\n",
    "    \n",
    "    subject_pun_c_pro = []\n",
    "    for i in range(0 ,len(df)):\n",
    "        if not subject_c_count[i] == 0:\n",
    "            subject_pun_c_pro.append(subject_pun_count[i] / subject_c_count[i])\n",
    "        else:\n",
    "            subject_pun_c_pro.append(0)\n",
    "    length['subject_pun_c_pro'] = subject_pun_c_pro\n",
    "            \n",
    "    content_pun_count = []\n",
    "    content_pun_w_pro = []\n",
    "    for item in df['content_word']:\n",
    "        i = 0\n",
    "        for word in item:\n",
    "            if word in string.punctuation:\n",
    "                i += 1\n",
    "        content_pun_count.append(i)\n",
    "        content_pun_w_pro.append(i / len(item))\n",
    "    length['content_pun_count'] = content_pun_count\n",
    "    length['content_pun_w_pro'] = content_pun_w_pro\n",
    "    \n",
    "    content_pun_c_pro = []\n",
    "    for i in range(0 ,len(df)):\n",
    "        if not content_c_count[i] == 0:\n",
    "            content_pun_c_pro.append(content_pun_count[i] / content_c_count[i])\n",
    "        else:\n",
    "            content_pun_c_pro.append(0)\n",
    "    length['content_pun_c_pro'] = content_pun_c_pro\n",
    "    \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23019f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = length_count(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4393083f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a2408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_c_count</th>\n",
       "      <th>subject_c_avg_w</th>\n",
       "      <th>subject_w_count</th>\n",
       "      <th>content_c_count</th>\n",
       "      <th>content_c_avg_w</th>\n",
       "      <th>content_w_count</th>\n",
       "      <th>content_c_avg_s</th>\n",
       "      <th>content_w_avg_s</th>\n",
       "      <th>content_s_count</th>\n",
       "      <th>subject_pun_count</th>\n",
       "      <th>subject_pun_w_pro</th>\n",
       "      <th>subject_pun_c_pro</th>\n",
       "      <th>content_pun_count</th>\n",
       "      <th>content_pun_w_pro</th>\n",
       "      <th>content_pun_c_pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>7</td>\n",
       "      <td>221</td>\n",
       "      <td>3.745763</td>\n",
       "      <td>59</td>\n",
       "      <td>44.200000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>11</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.049774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>15</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1947</td>\n",
       "      <td>3.559415</td>\n",
       "      <td>547</td>\n",
       "      <td>216.333333</td>\n",
       "      <td>60.777778</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81</td>\n",
       "      <td>0.148080</td>\n",
       "      <td>0.041602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>10</td>\n",
       "      <td>305</td>\n",
       "      <td>8.026316</td>\n",
       "      <td>38</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.003279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>239</td>\n",
       "      <td>3.676923</td>\n",
       "      <td>65</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.020921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_c_count  subject_c_avg_w  subject_w_count  content_c_count  \\\n",
       "0               27         3.857143                7              221   \n",
       "1               22         3.142857                7               43   \n",
       "2               11         5.500000                2             1947   \n",
       "3               43         4.300000               10              305   \n",
       "4               16         4.000000                4              239   \n",
       "\n",
       "   content_c_avg_w  content_w_count  content_c_avg_s  content_w_avg_s  \\\n",
       "0         3.745763               59        44.200000        11.800000   \n",
       "1         2.866667               15        21.500000         7.500000   \n",
       "2         3.559415              547       216.333333        60.777778   \n",
       "3         8.026316               38        30.500000         3.800000   \n",
       "4         3.676923               65        59.750000        16.250000   \n",
       "\n",
       "   content_s_count  subject_pun_count  subject_pun_w_pro  subject_pun_c_pro  \\\n",
       "0                5                  3           0.428571           0.111111   \n",
       "1                2                  1           0.142857           0.045455   \n",
       "2                9                  0           0.000000           0.000000   \n",
       "3               10                  4           0.400000           0.093023   \n",
       "4                4                  1           0.250000           0.062500   \n",
       "\n",
       "   content_pun_count  content_pun_w_pro  content_pun_c_pro  \n",
       "0                 11           0.186441           0.049774  \n",
       "1                  6           0.400000           0.139535  \n",
       "2                 81           0.148080           0.041602  \n",
       "3                  1           0.026316           0.003279  \n",
       "4                  5           0.076923           0.020921  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41845d",
   "metadata": {},
   "source": [
    "### Select the Best Feature\\*<br>\n",
    "\\* a failed try because it seems that the strong correlation between a feature and label does not necessarily mean that it is a good feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b9057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fe0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_train = []\n",
    "# for item in train_data['label']:\n",
    "#     if item == 'ham':\n",
    "#         label_train.append(0)\n",
    "#     elif item == 'spam':\n",
    "#         label_train.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7245c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_column = [column for column in length_train.columns]\n",
    "\n",
    "# length_column_need = []\n",
    "# for column in length_column:\n",
    "#     corr, p = stats.pearsonr([item for item in length_train[column]], label_train)\n",
    "#     if p >= 0.05:\n",
    "#         length_column_need.append(column)\n",
    "        \n",
    "# length_column_need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7666827",
   "metadata": {},
   "source": [
    "## 2-2. Second Type of Feature: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e46ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9630ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_subject = CountVectorizer()\n",
    "count_vect_content = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5519639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_train(df):\n",
    "        \n",
    "    subject_bow_train_counts = count_vect_subject.fit_transform(df['subject_lemma_concat'])\n",
    "    transformer_subject = TfidfTransformer().fit(subject_bow_train_counts)\n",
    "    subject_bow_train_tfidf = transformer_subject.transform(subject_bow_train_counts)\n",
    "    subject_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "        subject_bow_train_tfidf, columns = count_vect_subject.get_feature_names_out()\n",
    "    )\n",
    "    subject_tfidf = subject_tfidf.add_prefix('sj_')\n",
    "    \n",
    "    content_bow_train_counts = count_vect_content.fit_transform(df['content_lemma_concat'])\n",
    "    transformer_content = TfidfTransformer().fit(content_bow_train_counts)\n",
    "    content_bow_train_tfidf = transformer_content.transform(content_bow_train_counts)\n",
    "    content_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "        content_bow_train_tfidf, columns = count_vect_content.get_feature_names_out()\n",
    "    )\n",
    "    content_tfidf = content_tfidf.add_prefix('ct_')\n",
    "\n",
    "    bagofwords = pd.concat([subject_tfidf, content_tfidf], axis = 1)\n",
    "    \n",
    "    return bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd96b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_train = bag_of_words_train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "253bbde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 36378)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69fe44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sj_00</th>\n",
       "      <th>sj_000</th>\n",
       "      <th>sj_0004</th>\n",
       "      <th>sj_0067</th>\n",
       "      <th>sj_0071</th>\n",
       "      <th>sj_01</th>\n",
       "      <th>sj_01405</th>\n",
       "      <th>sj_02</th>\n",
       "      <th>sj_03</th>\n",
       "      <th>sj_0300</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_zxzmcnbf</th>\n",
       "      <th>ct_zyban</th>\n",
       "      <th>ct_zyjvit</th>\n",
       "      <th>ct_zyl</th>\n",
       "      <th>ct_zynsdirnh</th>\n",
       "      <th>ct_zynve</th>\n",
       "      <th>ct_zzezrjok</th>\n",
       "      <th>ct_zzn</th>\n",
       "      <th>ct_zzso</th>\n",
       "      <th>ct_zzsyt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sj_00  sj_000  sj_0004  sj_0067  sj_0071  sj_01  sj_01405  sj_02  sj_03  \\\n",
       "0    0.0     0.0      0.0      0.0      0.0    0.0       0.0    0.0    0.0   \n",
       "1    0.0     0.0      0.0      0.0      0.0    0.0       0.0    0.0    0.0   \n",
       "2    0.0     0.0      0.0      0.0      0.0    0.0       0.0    0.0    0.0   \n",
       "3    0.0     0.0      0.0      0.0      0.0    0.0       0.0    0.0    0.0   \n",
       "4    0.0     0.0      0.0      0.0      0.0    0.0       0.0    0.0    0.0   \n",
       "\n",
       "   sj_0300  ...  ct_zxzmcnbf  ct_zyban  ct_zyjvit  ct_zyl  ct_zynsdirnh  \\\n",
       "0      0.0  ...          0.0       0.0        0.0     0.0           0.0   \n",
       "1      0.0  ...          0.0       0.0        0.0     0.0           0.0   \n",
       "2      0.0  ...          0.0       0.0        0.0     0.0           0.0   \n",
       "3      0.0  ...          0.0       0.0        0.0     0.0           0.0   \n",
       "4      0.0  ...          0.0       0.0        0.0     0.0           0.0   \n",
       "\n",
       "   ct_zynve  ct_zzezrjok  ct_zzn  ct_zzso  ct_zzsyt  \n",
       "0       0.0          0.0     0.0      0.0       0.0  \n",
       "1       0.0          0.0     0.0      0.0       0.0  \n",
       "2       0.0          0.0     0.0      0.0       0.0  \n",
       "3       0.0          0.0     0.0      0.0       0.0  \n",
       "4       0.0          0.0     0.0      0.0       0.0  \n",
       "\n",
       "[5 rows x 36378 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da845a",
   "metadata": {},
   "source": [
    "## 2-3. Third Type of Feature: Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6d17524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da5ce941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_extract(df):\n",
    "    spam = df.loc[df['label'] == 'spam']\n",
    "    spam_subject_raw = ' '.join([text for text in spam['subject']])\n",
    "    spam_content_raw = ' '.join([text for text in spam['content']])\n",
    "    spam_subject_lemma = ' '.join([text for text in spam['subject_lemma_concat']])\n",
    "    spam_content_lemma = ' '.join([text for text in spam['content_lemma_concat']])\n",
    "    spam_full_raw = ' '.join([text for text in spam['full_text_concat']])\n",
    "    spam_full_lemma = ' '.join([text for text in spam['full_lemma_concat']])\n",
    "    return [spam_subject_raw, spam_content_raw, spam_subject_lemma, spam_content_lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c87edf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_text = spam_extract(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c24d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_sim_subject_raw = CountVectorizer()\n",
    "count_vect_sim_content_raw = CountVectorizer()\n",
    "count_vect_sim_subject_lemma = CountVectorizer()\n",
    "count_vect_sim_content_lemma = CountVectorizer()\n",
    "count_vect_sim_full_raw = CountVectorizer()\n",
    "count_vect_sim_full_lemma = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bfe5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_vector_train(df):\n",
    "    \n",
    "    subject_raw_vect = count_vect_sim_subject_raw.fit_transform([spam_text[0]] + [text for text in df['subject']])\n",
    "    subject_lemma_vect = count_vect_sim_subject_lemma.fit_transform(\n",
    "        [spam_text[2]] + [text for text in df['subject_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    content_raw_vect = count_vect_sim_content_raw.fit_transform([spam_text[1]] + [text for text in df['content']])\n",
    "    content_lemma_vect = count_vect_sim_content_lemma.fit_transform(\n",
    "        [spam_text[3]] + [text for text in df['content_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    full_raw_vect = count_vect_sim_full_raw.fit_transform([spam_text[1]] + [text for text in df['full_text_concat']])\n",
    "    full_lemma_vect = count_vect_sim_full_lemma.fit_transform(\n",
    "        [spam_text[3]] + [text for text in df['full_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    return [subject_raw_vect, content_raw_vect, subject_lemma_vect, content_lemma_vect, full_raw_vect, full_lemma_vect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cc90d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_vect_train = similarity_vector_train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a633c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_similarity(vect):\n",
    "    return [\n",
    "        cosine_similarity(vect[0], vect[i])[0][0] \n",
    "        for i in range(1, vect.shape[0])\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a55aee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_df(similarity_vect):\n",
    "    \n",
    "    similarity = pd.DataFrame()\n",
    "    \n",
    "    similarity['subject_raw_sim'] = compute_cos_similarity(similarity_vect[0])\n",
    "    similarity['content_raw_sim'] = compute_cos_similarity(similarity_vect[1])\n",
    "    similarity['subject_lemma_sim'] = compute_cos_similarity(similarity_vect[2])\n",
    "    similarity['content_lemma_sim'] = compute_cos_similarity(similarity_vect[3])\n",
    "    similarity['full_raw_sim'] = compute_cos_similarity(similarity_vect[4])\n",
    "    similarity['full_lemma_sim'] = compute_cos_similarity(similarity_vect[5])\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5400e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_train = similarity_df(similarity_vect_train).multiply(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abc23d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1de37a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_raw_sim</th>\n",
       "      <th>content_raw_sim</th>\n",
       "      <th>subject_lemma_sim</th>\n",
       "      <th>content_lemma_sim</th>\n",
       "      <th>full_raw_sim</th>\n",
       "      <th>full_lemma_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.188900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.289697</td>\n",
       "      <td>37.788841</td>\n",
       "      <td>8.708504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.853957</td>\n",
       "      <td>0.438871</td>\n",
       "      <td>1.261484</td>\n",
       "      <td>0.619363</td>\n",
       "      <td>4.276629</td>\n",
       "      <td>0.676846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.179939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.960519</td>\n",
       "      <td>81.019409</td>\n",
       "      <td>19.700789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.254263</td>\n",
       "      <td>0.126803</td>\n",
       "      <td>14.763301</td>\n",
       "      <td>0.927616</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>2.811889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.762176</td>\n",
       "      <td>40.069839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.601660</td>\n",
       "      <td>39.529842</td>\n",
       "      <td>12.399019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_raw_sim  content_raw_sim  subject_lemma_sim  content_lemma_sim  \\\n",
       "0         0.000000        39.188900           0.000000           9.289697   \n",
       "1        10.853957         0.438871           1.261484           0.619363   \n",
       "2         0.000000        81.179939           0.000000          19.960519   \n",
       "3         7.254263         0.126803          14.763301           0.927616   \n",
       "4        13.762176        40.069839           0.000000          12.601660   \n",
       "\n",
       "   full_raw_sim  full_lemma_sim  \n",
       "0     37.788841        8.708504  \n",
       "1      4.276629        0.676846  \n",
       "2     81.019409       19.700789  \n",
       "3      0.657822        2.811889  \n",
       "4     39.529842       12.399019  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094771dc",
   "metadata": {},
   "source": [
    "## 2-4. Concat Three Type of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c05d1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_concat(length, bag_of_words, similarity):\n",
    "    return pd.concat([length, bag_of_words, similarity], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58929e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = feature_concat(length_train, bag_of_words_train, similarity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c02cb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 36399)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "565ced00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_c_count</th>\n",
       "      <th>subject_c_avg_w</th>\n",
       "      <th>subject_w_count</th>\n",
       "      <th>content_c_count</th>\n",
       "      <th>content_c_avg_w</th>\n",
       "      <th>content_w_count</th>\n",
       "      <th>content_c_avg_s</th>\n",
       "      <th>content_w_avg_s</th>\n",
       "      <th>content_s_count</th>\n",
       "      <th>subject_pun_count</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_zzezrjok</th>\n",
       "      <th>ct_zzn</th>\n",
       "      <th>ct_zzso</th>\n",
       "      <th>ct_zzsyt</th>\n",
       "      <th>subject_raw_sim</th>\n",
       "      <th>content_raw_sim</th>\n",
       "      <th>subject_lemma_sim</th>\n",
       "      <th>content_lemma_sim</th>\n",
       "      <th>full_raw_sim</th>\n",
       "      <th>full_lemma_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>7</td>\n",
       "      <td>221</td>\n",
       "      <td>3.745763</td>\n",
       "      <td>59</td>\n",
       "      <td>44.200000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.188900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.289697</td>\n",
       "      <td>37.788841</td>\n",
       "      <td>8.708504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>15</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.853957</td>\n",
       "      <td>0.438871</td>\n",
       "      <td>1.261484</td>\n",
       "      <td>0.619363</td>\n",
       "      <td>4.276629</td>\n",
       "      <td>0.676846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1947</td>\n",
       "      <td>3.559415</td>\n",
       "      <td>547</td>\n",
       "      <td>216.333333</td>\n",
       "      <td>60.777778</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.179939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.960519</td>\n",
       "      <td>81.019409</td>\n",
       "      <td>19.700789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>10</td>\n",
       "      <td>305</td>\n",
       "      <td>8.026316</td>\n",
       "      <td>38</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.254263</td>\n",
       "      <td>0.126803</td>\n",
       "      <td>14.763301</td>\n",
       "      <td>0.927616</td>\n",
       "      <td>0.657822</td>\n",
       "      <td>2.811889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>239</td>\n",
       "      <td>3.676923</td>\n",
       "      <td>65</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.762176</td>\n",
       "      <td>40.069839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.601660</td>\n",
       "      <td>39.529842</td>\n",
       "      <td>12.399019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36399 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_c_count  subject_c_avg_w  subject_w_count  content_c_count  \\\n",
       "0               27         3.857143                7              221   \n",
       "1               22         3.142857                7               43   \n",
       "2               11         5.500000                2             1947   \n",
       "3               43         4.300000               10              305   \n",
       "4               16         4.000000                4              239   \n",
       "\n",
       "   content_c_avg_w  content_w_count  content_c_avg_s  content_w_avg_s  \\\n",
       "0         3.745763               59        44.200000        11.800000   \n",
       "1         2.866667               15        21.500000         7.500000   \n",
       "2         3.559415              547       216.333333        60.777778   \n",
       "3         8.026316               38        30.500000         3.800000   \n",
       "4         3.676923               65        59.750000        16.250000   \n",
       "\n",
       "   content_s_count  subject_pun_count  ...  ct_zzezrjok  ct_zzn  ct_zzso  \\\n",
       "0                5                  3  ...          0.0     0.0      0.0   \n",
       "1                2                  1  ...          0.0     0.0      0.0   \n",
       "2                9                  0  ...          0.0     0.0      0.0   \n",
       "3               10                  4  ...          0.0     0.0      0.0   \n",
       "4                4                  1  ...          0.0     0.0      0.0   \n",
       "\n",
       "   ct_zzsyt  subject_raw_sim  content_raw_sim  subject_lemma_sim  \\\n",
       "0       0.0         0.000000        39.188900           0.000000   \n",
       "1       0.0        10.853957         0.438871           1.261484   \n",
       "2       0.0         0.000000        81.179939           0.000000   \n",
       "3       0.0         7.254263         0.126803          14.763301   \n",
       "4       0.0        13.762176        40.069839           0.000000   \n",
       "\n",
       "   content_lemma_sim  full_raw_sim  full_lemma_sim  \n",
       "0           9.289697     37.788841        8.708504  \n",
       "1           0.619363      4.276629        0.676846  \n",
       "2          19.960519     81.019409       19.700789  \n",
       "3           0.927616      0.657822        2.811889  \n",
       "4          12.601660     39.529842       12.399019  \n",
       "\n",
       "[5 rows x 36399 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "997bc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_num = features_train.to_numpy()\n",
    "length_train_num = length_train.to_numpy()\n",
    "bag_of_words_train_num = bag_of_words_train.to_numpy()\n",
    "similarity_train_num = similarity_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473f084",
   "metadata": {},
   "source": [
    "## 2-5. Feature Extraction of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "285645aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_test(df):\n",
    "        \n",
    "    subject_bow_train_counts = count_vect_subject.transform(df['subject_lemma_concat'])\n",
    "    transformer_subject = TfidfTransformer().fit(subject_bow_train_counts)\n",
    "    subject_bow_train_tfidf = transformer_subject.transform(subject_bow_train_counts)\n",
    "    subject_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "        subject_bow_train_tfidf, columns = count_vect_subject.get_feature_names_out()\n",
    "    )\n",
    "    subject_tfidf = subject_tfidf.add_prefix('sj_')\n",
    "    \n",
    "    content_bow_train_counts = count_vect_content.transform(df['content_lemma_concat'])\n",
    "    transformer_content = TfidfTransformer().fit(content_bow_train_counts)\n",
    "    content_bow_train_tfidf = transformer_content.transform(content_bow_train_counts)\n",
    "    content_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "        content_bow_train_tfidf, columns = count_vect_content.get_feature_names_out()\n",
    "    )\n",
    "    content_tfidf = content_tfidf.add_prefix('ct_')\n",
    "\n",
    "    bagofwords = pd.concat([subject_tfidf, content_tfidf], axis = 1)\n",
    "    \n",
    "    return bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53c15d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_vector_test(df):\n",
    "    \n",
    "    subject_raw_vect = count_vect_sim_subject_raw.transform([spam_text[0]] + [text for text in df['subject']])\n",
    "    subject_lemma_vect = count_vect_sim_subject_lemma.transform(\n",
    "        [spam_text[2]] + [text for text in df['subject_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    content_raw_vect = count_vect_sim_content_raw.transform([spam_text[1]] + [text for text in df['content']])\n",
    "    content_lemma_vect = count_vect_sim_content_lemma.transform(\n",
    "        [spam_text[3]] + [text for text in df['content_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    full_raw_vect = count_vect_sim_full_raw.transform([spam_text[1]] + [text for text in df['full_text_concat']])\n",
    "    full_lemma_vect = count_vect_sim_full_lemma.transform(\n",
    "        [spam_text[3]] + [text for text in df['full_lemma_concat']]\n",
    "    )\n",
    "    \n",
    "    return [subject_raw_vect, content_raw_vect, subject_lemma_vect, content_lemma_vect, full_raw_vect, full_lemma_vect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "703aaba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('part2-data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "891f49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd8a3c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 15)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "187aa39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_word</th>\n",
       "      <th>subject_lemma</th>\n",
       "      <th>subject_lemma_concat</th>\n",
       "      <th>content</th>\n",
       "      <th>content_word</th>\n",
       "      <th>content_lemma</th>\n",
       "      <th>content_lemma_concat</th>\n",
       "      <th>content_sent</th>\n",
       "      <th>full_text</th>\n",
       "      <th>full_text_concat</th>\n",
       "      <th>full_lemma</th>\n",
       "      <th>full_lemma_concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: enron / hpl actuals for november 20 ,...</td>\n",
       "      <td>ham</td>\n",
       "      <td>enron / hpl actuals for november 20 , 2000</td>\n",
       "      <td>[enron, /, hpl, actuals, for, november, 20, ,,...</td>\n",
       "      <td>[enron, hpl, actuals, november, 20, 2000]</td>\n",
       "      <td>enron hpl actuals november 20 2000</td>\n",
       "      <td>teco tap 30 . 000 / enron ; 68 . 542 / hpl gas...</td>\n",
       "      <td>[teco, tap, 30, ., 000, /, enron, ;, 68, ., 54...</td>\n",
       "      <td>[teco, tap, 30, 000, enron, 68, 542, hpl, gas,...</td>\n",
       "      <td>teco tap 30 000 enron 68 542 hpl gas daily</td>\n",
       "      <td>[[teco, tap, 30, ., 000, /, enron, ;, 68, ., 5...</td>\n",
       "      <td>[enron, /, hpl, actuals, for, november, 20, ,,...</td>\n",
       "      <td>enron / hpl actuals for november 20 , 2000 tec...</td>\n",
       "      <td>[enron, hpl, actuals, november, 20, 2000, teco...</td>\n",
       "      <td>enron hpl actuals november 20 2000 teco tap 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: defs purchase of teco pipeline\\r\\neff...</td>\n",
       "      <td>ham</td>\n",
       "      <td>defs purchase of teco pipeline</td>\n",
       "      <td>[defs, purchase, of, teco, pipeline]</td>\n",
       "      <td>[defs, purchase, teco, pipeline]</td>\n",
       "      <td>defs purchase teco pipeline</td>\n",
       "      <td>effective february 1 , 2001 duke energy field ...</td>\n",
       "      <td>[effective, february, 1, ,, 2001, duke, energy...</td>\n",
       "      <td>[effective, february, 1, 2001, duke, energy, f...</td>\n",
       "      <td>effective february 1 2001 duke energy field se...</td>\n",
       "      <td>[[effective, february, 1, ,, 2001, duke, energ...</td>\n",
       "      <td>[defs, purchase, of, teco, pipeline, effective...</td>\n",
       "      <td>defs purchase of teco pipeline effective febru...</td>\n",
       "      <td>[defs, purchase, teco, pipeline, effective, fe...</td>\n",
       "      <td>defs purchase teco pipeline effective february...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: your son knows you watch girls finger...</td>\n",
       "      <td>spam</td>\n",
       "      <td>your son knows you watch girls fingering their...</td>\n",
       "      <td>[your, son, knows, you, watch, girls, fingerin...</td>\n",
       "      <td>[son, know, watch, girl, finger, asshole]</td>\n",
       "      <td>son know watch girl finger asshole</td>\n",
       "      <td>remove campanile dittyutile portent blatarchfo...</td>\n",
       "      <td>[remove, campanile, dittyutile, portent, blata...</td>\n",
       "      <td>[remove, campanile, dittyutile, portent, blata...</td>\n",
       "      <td>remove campanile dittyutile portent blatarchfo...</td>\n",
       "      <td>[[remove], [campanile, dittyutile, portent, bl...</td>\n",
       "      <td>[your, son, knows, you, watch, girls, fingerin...</td>\n",
       "      <td>your son knows you watch girls fingering their...</td>\n",
       "      <td>[son, know, watch, girl, finger, asshole, remo...</td>\n",
       "      <td>son know watch girl finger asshole remove camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: eex corporation - meter # 5999\\r\\nsit...</td>\n",
       "      <td>ham</td>\n",
       "      <td>eex corporation - meter # 5999</td>\n",
       "      <td>[eex, corporation, -, meter, #, 5999]</td>\n",
       "      <td>[eex, corporation, meter, 5999]</td>\n",
       "      <td>eex corporation meter 5999</td>\n",
       "      <td>sitara deal ticket # 314349 has been created a...</td>\n",
       "      <td>[sitara, deal, ticket, #, 314349, has, been, c...</td>\n",
       "      <td>[sitara, deal, ticket, 314349, create, enter, ...</td>\n",
       "      <td>sitara deal ticket 314349 create enter july 20...</td>\n",
       "      <td>[[sitara, deal, ticket, #, 314349, has, been, ...</td>\n",
       "      <td>[eex, corporation, -, meter, #, 5999, sitara, ...</td>\n",
       "      <td>eex corporation - meter # 5999 sitara deal tic...</td>\n",
       "      <td>[eex, corporation, meter, 5999, sitara, deal, ...</td>\n",
       "      <td>eex corporation meter 5999 sitara deal ticket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: deal 93481\\r\\ndaren ,\\r\\ni ' m lookin...</td>\n",
       "      <td>ham</td>\n",
       "      <td>deal 93481</td>\n",
       "      <td>[deal, 93481]</td>\n",
       "      <td>[deal, 93481]</td>\n",
       "      <td>deal 93481</td>\n",
       "      <td>daren , i ' m looking into this deal . there i...</td>\n",
       "      <td>[daren, ,, i, ', m, looking, into, this, deal,...</td>\n",
       "      <td>[daren, m, look, deal, volume, deal, january, ...</td>\n",
       "      <td>daren m look deal volume deal january 00 deal ...</td>\n",
       "      <td>[[daren, ,], [i, ', m, looking, into, this, de...</td>\n",
       "      <td>[deal, 93481, daren, ,, i, ', m, looking, into...</td>\n",
       "      <td>deal 93481 daren , i ' m looking into this dea...</td>\n",
       "      <td>[deal, 93481, daren, m, look, deal, volume, de...</td>\n",
       "      <td>deal 93481 daren m look deal volume deal janua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Subject: enron / hpl actuals for november 20 ,...   ham   \n",
       "1  Subject: defs purchase of teco pipeline\\r\\neff...   ham   \n",
       "2  Subject: your son knows you watch girls finger...  spam   \n",
       "3  Subject: eex corporation - meter # 5999\\r\\nsit...   ham   \n",
       "4  Subject: deal 93481\\r\\ndaren ,\\r\\ni ' m lookin...   ham   \n",
       "\n",
       "                                             subject  \\\n",
       "0         enron / hpl actuals for november 20 , 2000   \n",
       "1                     defs purchase of teco pipeline   \n",
       "2  your son knows you watch girls fingering their...   \n",
       "3                     eex corporation - meter # 5999   \n",
       "4                                         deal 93481   \n",
       "\n",
       "                                        subject_word  \\\n",
       "0  [enron, /, hpl, actuals, for, november, 20, ,,...   \n",
       "1               [defs, purchase, of, teco, pipeline]   \n",
       "2  [your, son, knows, you, watch, girls, fingerin...   \n",
       "3              [eex, corporation, -, meter, #, 5999]   \n",
       "4                                      [deal, 93481]   \n",
       "\n",
       "                               subject_lemma  \\\n",
       "0  [enron, hpl, actuals, november, 20, 2000]   \n",
       "1           [defs, purchase, teco, pipeline]   \n",
       "2  [son, know, watch, girl, finger, asshole]   \n",
       "3            [eex, corporation, meter, 5999]   \n",
       "4                              [deal, 93481]   \n",
       "\n",
       "                 subject_lemma_concat  \\\n",
       "0  enron hpl actuals november 20 2000   \n",
       "1         defs purchase teco pipeline   \n",
       "2  son know watch girl finger asshole   \n",
       "3          eex corporation meter 5999   \n",
       "4                          deal 93481   \n",
       "\n",
       "                                             content  \\\n",
       "0  teco tap 30 . 000 / enron ; 68 . 542 / hpl gas...   \n",
       "1  effective february 1 , 2001 duke energy field ...   \n",
       "2  remove campanile dittyutile portent blatarchfo...   \n",
       "3  sitara deal ticket # 314349 has been created a...   \n",
       "4  daren , i ' m looking into this deal . there i...   \n",
       "\n",
       "                                        content_word  \\\n",
       "0  [teco, tap, 30, ., 000, /, enron, ;, 68, ., 54...   \n",
       "1  [effective, february, 1, ,, 2001, duke, energy...   \n",
       "2  [remove, campanile, dittyutile, portent, blata...   \n",
       "3  [sitara, deal, ticket, #, 314349, has, been, c...   \n",
       "4  [daren, ,, i, ', m, looking, into, this, deal,...   \n",
       "\n",
       "                                       content_lemma  \\\n",
       "0  [teco, tap, 30, 000, enron, 68, 542, hpl, gas,...   \n",
       "1  [effective, february, 1, 2001, duke, energy, f...   \n",
       "2  [remove, campanile, dittyutile, portent, blata...   \n",
       "3  [sitara, deal, ticket, 314349, create, enter, ...   \n",
       "4  [daren, m, look, deal, volume, deal, january, ...   \n",
       "\n",
       "                                content_lemma_concat  \\\n",
       "0         teco tap 30 000 enron 68 542 hpl gas daily   \n",
       "1  effective february 1 2001 duke energy field se...   \n",
       "2  remove campanile dittyutile portent blatarchfo...   \n",
       "3  sitara deal ticket 314349 create enter july 20...   \n",
       "4  daren m look deal volume deal january 00 deal ...   \n",
       "\n",
       "                                        content_sent  \\\n",
       "0  [[teco, tap, 30, ., 000, /, enron, ;, 68, ., 5...   \n",
       "1  [[effective, february, 1, ,, 2001, duke, energ...   \n",
       "2  [[remove], [campanile, dittyutile, portent, bl...   \n",
       "3  [[sitara, deal, ticket, #, 314349, has, been, ...   \n",
       "4  [[daren, ,], [i, ', m, looking, into, this, de...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  [enron, /, hpl, actuals, for, november, 20, ,,...   \n",
       "1  [defs, purchase, of, teco, pipeline, effective...   \n",
       "2  [your, son, knows, you, watch, girls, fingerin...   \n",
       "3  [eex, corporation, -, meter, #, 5999, sitara, ...   \n",
       "4  [deal, 93481, daren, ,, i, ', m, looking, into...   \n",
       "\n",
       "                                    full_text_concat  \\\n",
       "0  enron / hpl actuals for november 20 , 2000 tec...   \n",
       "1  defs purchase of teco pipeline effective febru...   \n",
       "2  your son knows you watch girls fingering their...   \n",
       "3  eex corporation - meter # 5999 sitara deal tic...   \n",
       "4  deal 93481 daren , i ' m looking into this dea...   \n",
       "\n",
       "                                          full_lemma  \\\n",
       "0  [enron, hpl, actuals, november, 20, 2000, teco...   \n",
       "1  [defs, purchase, teco, pipeline, effective, fe...   \n",
       "2  [son, know, watch, girl, finger, asshole, remo...   \n",
       "3  [eex, corporation, meter, 5999, sitara, deal, ...   \n",
       "4  [deal, 93481, daren, m, look, deal, volume, de...   \n",
       "\n",
       "                                   full_lemma_concat  \n",
       "0  enron hpl actuals november 20 2000 teco tap 30...  \n",
       "1  defs purchase teco pipeline effective february...  \n",
       "2  son know watch girl finger asshole remove camp...  \n",
       "3  eex corporation meter 5999 sitara deal ticket ...  \n",
       "4  deal 93481 daren m look deal volume deal janua...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cbe885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_test = length_count(test_data)\n",
    "bag_of_words_test = bag_of_words_test(test_data)\n",
    "\n",
    "similarity_vect_test = similarity_vector_test(test_data)\n",
    "similarity_test = similarity_df(similarity_vect_test).multiply(100)\n",
    "\n",
    "features_test = feature_concat(length_test, bag_of_words_test, similarity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5e7865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 36399)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4eb1801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_c_count</th>\n",
       "      <th>subject_c_avg_w</th>\n",
       "      <th>subject_w_count</th>\n",
       "      <th>content_c_count</th>\n",
       "      <th>content_c_avg_w</th>\n",
       "      <th>content_w_count</th>\n",
       "      <th>content_c_avg_s</th>\n",
       "      <th>content_w_avg_s</th>\n",
       "      <th>content_s_count</th>\n",
       "      <th>subject_pun_count</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_zzezrjok</th>\n",
       "      <th>ct_zzn</th>\n",
       "      <th>ct_zzso</th>\n",
       "      <th>ct_zzsyt</th>\n",
       "      <th>subject_raw_sim</th>\n",
       "      <th>content_raw_sim</th>\n",
       "      <th>subject_lemma_sim</th>\n",
       "      <th>content_lemma_sim</th>\n",
       "      <th>full_raw_sim</th>\n",
       "      <th>full_lemma_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>15</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.173268</td>\n",
       "      <td>1.075011</td>\n",
       "      <td>1.029998</td>\n",
       "      <td>3.791650</td>\n",
       "      <td>4.934092</td>\n",
       "      <td>4.213159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>1215</td>\n",
       "      <td>3.944805</td>\n",
       "      <td>308</td>\n",
       "      <td>41.896552</td>\n",
       "      <td>10.620690</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.365185</td>\n",
       "      <td>53.366387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.793772</td>\n",
       "      <td>53.476400</td>\n",
       "      <td>12.539463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>9</td>\n",
       "      <td>725</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>75</td>\n",
       "      <td>34.523810</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.881719</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>7.898142</td>\n",
       "      <td>2.568845</td>\n",
       "      <td>6.758467</td>\n",
       "      <td>4.253398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>3.275862</td>\n",
       "      <td>29</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.656883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.133573</td>\n",
       "      <td>14.343612</td>\n",
       "      <td>4.167947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>289</td>\n",
       "      <td>3.612500</td>\n",
       "      <td>80</td>\n",
       "      <td>41.285714</td>\n",
       "      <td>11.428571</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306457</td>\n",
       "      <td>42.806998</td>\n",
       "      <td>1.189339</td>\n",
       "      <td>10.293540</td>\n",
       "      <td>41.429493</td>\n",
       "      <td>9.828983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36399 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_c_count  subject_c_avg_w  subject_w_count  content_c_count  \\\n",
       "0               34         3.777778                9               38   \n",
       "1               26         5.200000                5             1215   \n",
       "2               46         5.111111                9              725   \n",
       "3               25         4.166667                6               95   \n",
       "4                9         4.500000                2              289   \n",
       "\n",
       "   content_c_avg_w  content_w_count  content_c_avg_s  content_w_avg_s  \\\n",
       "0         2.533333               15        38.000000        15.000000   \n",
       "1         3.944805              308        41.896552        10.620690   \n",
       "2         9.666667               75        34.523810         3.571429   \n",
       "3         3.275862               29        23.750000         7.250000   \n",
       "4         3.612500               80        41.285714        11.428571   \n",
       "\n",
       "   content_s_count  subject_pun_count  ...  ct_zzezrjok  ct_zzn  ct_zzso  \\\n",
       "0                1                  2  ...          0.0     0.0      0.0   \n",
       "1               29                  0  ...          0.0     0.0      0.0   \n",
       "2               21                  0  ...          0.0     0.0      0.0   \n",
       "3                4                  2  ...          0.0     0.0      0.0   \n",
       "4                7                  0  ...          0.0     0.0      0.0   \n",
       "\n",
       "   ct_zzsyt  subject_raw_sim  content_raw_sim  subject_lemma_sim  \\\n",
       "0       0.0         9.173268         1.075011           1.029998   \n",
       "1       0.0         7.365185        53.366387           0.000000   \n",
       "2       0.0        25.881719         0.454217           7.898142   \n",
       "3       0.0         0.000000        15.656883           0.000000   \n",
       "4       0.0         0.306457        42.806998           1.189339   \n",
       "\n",
       "   content_lemma_sim  full_raw_sim  full_lemma_sim  \n",
       "0           3.791650      4.934092        4.213159  \n",
       "1          12.793772     53.476400       12.539463  \n",
       "2           2.568845      6.758467        4.253398  \n",
       "3           4.133573     14.343612        4.167947  \n",
       "4          10.293540     41.429493        9.828983  \n",
       "\n",
       "[5 rows x 36399 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e22be254",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_num = features_test.to_numpy()\n",
    "length_test_num = length_test.to_numpy()\n",
    "bag_of_words_test_num = bag_of_words_test.to_numpy()\n",
    "similarity_test_num = similarity_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56366c2",
   "metadata": {},
   "source": [
    "# 3. Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be4dbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61879337",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = [feature for feature in features_test.columns]\n",
    "features_type = []\n",
    "for feature in features_name:\n",
    "    if feature in [column for column in length_test.columns]:\n",
    "        features_type.append('length')\n",
    "    elif feature in [column for column in similarity_test.columns]:\n",
    "        features_type.append('similarity')\n",
    "    else:\n",
    "        features_type.append('bag of words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed2123",
   "metadata": {},
   "source": [
    "## 3-1. Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fe048c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f4240",
   "metadata": {},
   "source": [
    "#### Cross-Validation (Train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6dcf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy = \"most_frequent\").fit(features_train_num, train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d73e97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_scores = cross_val_score(dummy_clf, features_train_num, train_data['label'], cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e07b6900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7240000000000001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dummy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88538507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015996289969864887"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.96 * sqrt((np.mean(dummy_scores) * (1 - np.mean(dummy_scores))) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228d3ea",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0755091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dummy = dummy_clf.predict(features_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b2d74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dummy = dummy_clf.score(features_test_num, test_data['label'])\n",
    "precision_dummy = precision_score(test_data['label'], y_pred_dummy, average = 'macro')\n",
    "recall_dummy = recall_score(test_data['label'], y_pred_dummy, pos_label = 'spam')\n",
    "f1_dummy = f1_score(test_data['label'], y_pred_dummy, pos_label = 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09c6b6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6909258406264395, 0.34546292031321973, 0.0, 0.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dummy, precision_dummy, recall_dummy, f1_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e352b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dummy_interval = 1.96 * sqrt((accuracy_dummy * (1 - accuracy_dummy)) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74c6813b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019438968937633017"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dummy_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65020287",
   "metadata": {},
   "source": [
    "## 3-2. Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f96a3",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b4f4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f876d",
   "metadata": {},
   "source": [
    "#### Cross-Validation (Train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76e59bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_clf = LogisticRegression().fit(features_train_num, train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b843759",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_scores = cross_val_score(LR_clf, features_train_num, train_data['label'], cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "717f8f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.857"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(LR_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd3d2c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012527196036357591"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.96 * sqrt((np.mean(LR_scores) * (1 - np.mean(LR_scores))) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509eca2",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1de4ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LR = LR_clf.predict(features_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2b24ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LR = LR_clf.score(features_test_num, test_data['label'])\n",
    "precision_LR = precision_score(test_data['label'], y_pred_LR, average = 'macro')\n",
    "recall_LR = recall_score(test_data['label'], y_pred_LR, pos_label = 'spam')\n",
    "f1_LR = f1_score(test_data['label'], y_pred_LR, pos_label = 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e99bed4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8420082911100876,\n",
       " 0.8328891580860085,\n",
       " 0.6318926974664679,\n",
       " 0.7120067170445002)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LR, precision_LR, recall_LR, f1_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e885631",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LR_interval = 1.96 * sqrt((accuracy_LR * (1 - accuracy_LR)) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa87f234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015342689353946625"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LR_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ef39d",
   "metadata": {},
   "source": [
    "#### Contribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca05a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36395</th>\n",
       "      <td>subject_lemma_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.108949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject_c_avg_w</td>\n",
       "      <td>length</td>\n",
       "      <td>0.052580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>content_w_avg_s</td>\n",
       "      <td>length</td>\n",
       "      <td>0.052439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content_c_avg_w</td>\n",
       "      <td>length</td>\n",
       "      <td>0.033101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36398</th>\n",
       "      <td>full_lemma_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36396</th>\n",
       "      <td>content_lemma_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.029338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>subject_pun_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.019045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36394</th>\n",
       "      <td>content_raw_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.018344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>content_c_avg_s</td>\n",
       "      <td>length</td>\n",
       "      <td>0.013934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject_c_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.012296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature        Type  Importance\n",
       "36395  subject_lemma_sim  similarity    0.108949\n",
       "1        subject_c_avg_w      length    0.052580\n",
       "7        content_w_avg_s      length    0.052439\n",
       "4        content_c_avg_w      length    0.033101\n",
       "36398     full_lemma_sim  similarity    0.032500\n",
       "36396  content_lemma_sim  similarity    0.029338\n",
       "9      subject_pun_count      length    0.019045\n",
       "36394    content_raw_sim  similarity    0.018344\n",
       "6        content_c_avg_s      length    0.013934\n",
       "0        subject_c_count      length    0.012296"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_LR = pd.DataFrame()\n",
    "feature_importance_LR['Feature'] = features_name\n",
    "feature_importance_LR['Type'] = features_type\n",
    "feature_importance_LR_sum = sum([np.abs(num) for num in LR_clf.coef_[0]])\n",
    "feature_importance_LR['Importance'] = [np.abs(num) / feature_importance_LR_sum for num in LR_clf.coef_[0]]\n",
    "feature_importance_LR.sort_values(by = 'Importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b8af8",
   "metadata": {},
   "source": [
    "##### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "791f49a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7632427452786734,\n",
       " 0.8260290148448044,\n",
       " 0.26229508196721313,\n",
       " 0.40646651270207856)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_clf_length = LogisticRegression().fit(length_train_num, train_data['label'])\n",
    "y_pred_LR_length = LR_clf_length.predict(length_test_num)\n",
    "accuracy_LR_length = LR_clf_length.score(length_test_num, test_data['label'])\n",
    "precision_LR_length = precision_score(test_data['label'], y_pred_LR_length, average = 'macro')\n",
    "recall_LR_length = recall_score(test_data['label'], y_pred_LR_length, pos_label = 'spam')\n",
    "f1_LR_length = f1_score(test_data['label'], y_pred_LR_length, pos_label = 'spam')\n",
    "accuracy_LR_length, precision_LR_length, recall_LR_length, f1_LR_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8f98f",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "feae0b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9843390142791341,\n",
       " 0.9820498428602421,\n",
       " 0.9731743666169895,\n",
       " 0.9746268656716417)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_clf_bow = LogisticRegression().fit(bag_of_words_train_num, train_data['label'])\n",
    "y_pred_LR_bow = LR_clf_bow.predict(bag_of_words_test_num)\n",
    "accuracy_LR_bow = LR_clf_bow.score(bag_of_words_test_num, test_data['label'])\n",
    "precision_LR_bow = precision_score(test_data['label'], y_pred_LR_bow, average = 'macro')\n",
    "recall_LR_bow = recall_score(test_data['label'], y_pred_LR_bow, pos_label = 'spam')\n",
    "f1_LR_bow = f1_score(test_data['label'], y_pred_LR_bow, pos_label = 'spam')\n",
    "accuracy_LR_bow, precision_LR_bow, recall_LR_bow, f1_LR_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c37ceb",
   "metadata": {},
   "source": [
    "##### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07d8bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8198986642100414,\n",
       " 0.8146889825302019,\n",
       " 0.5499254843517138,\n",
       " 0.6536758193091231)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_clf_sim = LogisticRegression().fit(similarity_train_num, train_data['label'])\n",
    "y_pred_LR_sim = LR_clf_sim.predict(similarity_test_num)\n",
    "accuracy_LR_sim = LR_clf_sim.score(similarity_test_num, test_data['label'])\n",
    "precision_LR_sim = precision_score(test_data['label'], y_pred_LR_sim, average = 'macro')\n",
    "recall_LR_sim = recall_score(test_data['label'], y_pred_LR_sim, pos_label = 'spam')\n",
    "f1_LR_sim = f1_score(test_data['label'], y_pred_LR_sim, pos_label = 'spam')\n",
    "accuracy_LR_sim, precision_LR_sim, recall_LR_sim, f1_LR_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4115af",
   "metadata": {},
   "source": [
    "## 3-3. Probabilistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa6afb",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "919fdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287710de",
   "metadata": {},
   "source": [
    "#### Cross Validation (Train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6fbfeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = MultinomialNB().fit(features_train_num, train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b29d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_scores = cross_val_score(NB_clf, features_train_num, train_data['label'], cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df7e59f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(NB_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88105e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012742462326149265"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.96 * sqrt((np.mean(NB_scores) * (1 - np.mean(NB_scores))) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b56834",
   "metadata": {},
   "source": [
    "#### Predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e573d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NB = NB_clf.predict(features_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "871e6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_NB = NB_clf.score(features_test_num, test_data['label'])\n",
    "precision_NB = precision_score(test_data['label'], y_pred_NB, average = 'macro')\n",
    "recall_NB = recall_score(test_data['label'], y_pred_NB, pos_label = 'spam')\n",
    "f1_NB = f1_score(test_data['label'], y_pred_NB, pos_label = 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "536eedd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8530631045601106,\n",
       " 0.8526906044284859,\n",
       " 0.6348733233979136,\n",
       " 0.7275832621690864)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_NB, precision_NB, recall_NB, f1_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "772eb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_NB_interval = 1.96 * sqrt((accuracy_NB * (1 - accuracy_NB)) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2254f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014892999292565825"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_NB_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de4d88",
   "metadata": {},
   "source": [
    "#### Contribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1041b95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content_c_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.594845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>content_w_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.162796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>content_pun_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.042845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>content_c_avg_s</td>\n",
       "      <td>length</td>\n",
       "      <td>0.036120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36397</th>\n",
       "      <td>full_raw_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.025615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36394</th>\n",
       "      <td>content_raw_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.025384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject_c_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.020164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>content_s_count</td>\n",
       "      <td>length</td>\n",
       "      <td>0.014405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>content_w_avg_s</td>\n",
       "      <td>length</td>\n",
       "      <td>0.009848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36398</th>\n",
       "      <td>full_lemma_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.008699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature        Type  Importance\n",
       "3        content_c_count      length    0.594845\n",
       "5        content_w_count      length    0.162796\n",
       "12     content_pun_count      length    0.042845\n",
       "6        content_c_avg_s      length    0.036120\n",
       "36397       full_raw_sim  similarity    0.025615\n",
       "36394    content_raw_sim  similarity    0.025384\n",
       "0        subject_c_count      length    0.020164\n",
       "8        content_s_count      length    0.014405\n",
       "7        content_w_avg_s      length    0.009848\n",
       "36398     full_lemma_sim  similarity    0.008699"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_NB = NB_clf.feature_log_prob_\n",
    "prob_NB = np.exp(log_prob_NB)\n",
    "feature_importances_NB = np.mean(prob_NB, axis = 0)\n",
    "feature_importances_NB_df = pd.DataFrame()\n",
    "feature_importances_NB_df['Feature'] = features_name\n",
    "feature_importances_NB_df['Type'] = features_type\n",
    "feature_importances_NB_df['Importance'] = feature_importances_NB\n",
    "feature_importances_NB_df.sort_values(by = 'Importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb7ef1",
   "metadata": {},
   "source": [
    "##### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06d28376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5679410409949333,\n",
       " 0.5633177593902216,\n",
       " 0.5901639344262295,\n",
       " 0.45780346820809253)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_length = MultinomialNB().fit(length_train_num, train_data['label'])\n",
    "y_pred_NB_length = NB_clf_length.predict(length_test_num)\n",
    "accuracy_NB_length = NB_clf_length.score(length_test_num, test_data['label'])\n",
    "precision_NB_length = precision_score(test_data['label'], y_pred_NB_length, average = 'macro')\n",
    "recall_NB_length = recall_score(test_data['label'], y_pred_NB_length, pos_label = 'spam')\n",
    "f1_NB_length = f1_score(test_data['label'], y_pred_NB_length, pos_label = 'spam')\n",
    "accuracy_NB_length, precision_NB_length, recall_NB_length, f1_NB_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac75d1f",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fad06e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9207738369415016,\n",
       " 0.9478043912175649,\n",
       " 0.7451564828614009,\n",
       " 0.8532423208191126)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_bow = MultinomialNB().fit(bag_of_words_train_num, train_data['label'])\n",
    "y_pred_NB_bow = NB_clf_bow.predict(bag_of_words_test_num)\n",
    "accuracy_NB_bow = NB_clf_bow.score(bag_of_words_test_num, test_data['label'])\n",
    "precision_NB_bow = precision_score(test_data['label'], y_pred_NB_bow, average = 'macro')\n",
    "recall_NB_bow = recall_score(test_data['label'], y_pred_NB_bow, pos_label = 'spam')\n",
    "f1_NB_bow = f1_score(test_data['label'], y_pred_NB_bow, pos_label = 'spam')\n",
    "accuracy_NB_bow, precision_NB_bow, recall_NB_bow, f1_NB_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9a1db",
   "metadata": {},
   "source": [
    "##### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42045fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7010594196222939,\n",
       " 0.6693778591843416,\n",
       " 0.6616989567809239,\n",
       " 0.5777488614183475)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_sim = MultinomialNB().fit(similarity_train_num, train_data['label'])\n",
    "y_pred_NB_sim = NB_clf_sim.predict(similarity_test_num)\n",
    "accuracy_NB_sim = NB_clf_sim.score(similarity_test_num, test_data['label'])\n",
    "precision_NB_sim = precision_score(test_data['label'], y_pred_NB_sim, average = 'macro')\n",
    "recall_NB_sim = recall_score(test_data['label'], y_pred_NB_sim, pos_label = 'spam')\n",
    "f1_NB_sim = f1_score(test_data['label'], y_pred_NB_sim, pos_label = 'spam')\n",
    "accuracy_NB_sim, precision_NB_sim, recall_NB_sim, f1_NB_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1138c39",
   "metadata": {},
   "source": [
    "## 3-4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "217b6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0646a",
   "metadata": {},
   "source": [
    "#### Cross Validation (Train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "012d0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_clf = DecisionTreeClassifier().fit(features_train_num, train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e83c96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_scores = cross_val_score(DT_clf, features_train_num, train_data['label'], cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e9b8517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9326666666666666"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(DT_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a4f46b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008967548394483714"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.96 * sqrt((np.mean(DT_scores) * (1 - np.mean(DT_scores))) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e0238",
   "metadata": {},
   "source": [
    "#### Predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0474ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DT = DT_clf.predict(features_test_num)\n",
    "accuracy_DT = DT_clf.score(features_test_num, test_data['label'])\n",
    "precision_DT = precision_score(test_data['label'], y_pred_DT, average = 'macro')\n",
    "recall_DT = recall_score(test_data['label'], y_pred_DT, pos_label = 'spam')\n",
    "f1_DT = f1_score(test_data['label'], y_pred_DT, pos_label = 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ae3f91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9253800092123445,\n",
       " 0.9235122420861476,\n",
       " 0.8315946348733234,\n",
       " 0.8732394366197183)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_DT, precision_DT, recall_DT, f1_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "07d1c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_DT_interval = 1.96 * sqrt((accuracy_DT * (1 - accuracy_DT)) / len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "be7e47cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009403360717751249"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_DT_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca7989",
   "metadata": {},
   "source": [
    "#### Contribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "69a2de49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36395</th>\n",
       "      <td>subject_lemma_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.245599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16030</th>\n",
       "      <td>ct_enron</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.124560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19792</th>\n",
       "      <td>ct_http</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.077835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sj_00</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.064877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject_c_avg_w</td>\n",
       "      <td>length</td>\n",
       "      <td>0.055112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content_c_avg_w</td>\n",
       "      <td>length</td>\n",
       "      <td>0.042098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36397</th>\n",
       "      <td>full_raw_sim</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.031733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32856</th>\n",
       "      <td>ct_thanks</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.022922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>content_w_avg_s</td>\n",
       "      <td>length</td>\n",
       "      <td>0.019794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>sj_new</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.018672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature          Type  Importance\n",
       "36395  subject_lemma_sim    similarity    0.245599\n",
       "16030           ct_enron  bag of words    0.124560\n",
       "19792            ct_http  bag of words    0.077835\n",
       "15                 sj_00  bag of words    0.064877\n",
       "1        subject_c_avg_w        length    0.055112\n",
       "4        content_c_avg_w        length    0.042098\n",
       "36397       full_raw_sim    similarity    0.031733\n",
       "32856          ct_thanks  bag of words    0.022922\n",
       "7        content_w_avg_s        length    0.019794\n",
       "2028              sj_new  bag of words    0.018672"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_DT = pd.DataFrame()\n",
    "feature_importance_DT['Feature'] = features_name\n",
    "feature_importance_DT['Type'] = features_type\n",
    "feature_importance_DT['Importance'] = [num for num in DT_clf.feature_importances_]\n",
    "feature_importance_DT.sort_values(by = 'Importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03153dd4",
   "metadata": {},
   "source": [
    "##### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "866735c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5679410409949333,\n",
       " 0.5633177593902216,\n",
       " 0.5901639344262295,\n",
       " 0.45780346820809253)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_length = MultinomialNB().fit(length_train_num, train_data['label'])\n",
    "y_pred_NB_length = NB_clf_length.predict(length_test_num)\n",
    "accuracy_NB_length = NB_clf_length.score(length_test_num, test_data['label'])\n",
    "precision_NB_length = precision_score(test_data['label'], y_pred_NB_length, average = 'macro')\n",
    "recall_NB_length = recall_score(test_data['label'], y_pred_NB_length, pos_label = 'spam')\n",
    "f1_NB_length = f1_score(test_data['label'], y_pred_NB_length, pos_label = 'spam')\n",
    "accuracy_NB_length, precision_NB_length, recall_NB_length, f1_NB_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59841add",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab123264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9207738369415016,\n",
       " 0.9478043912175649,\n",
       " 0.7451564828614009,\n",
       " 0.8532423208191126)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_bow = MultinomialNB().fit(bag_of_words_train_num, train_data['label'])\n",
    "y_pred_NB_bow = NB_clf_bow.predict(bag_of_words_test_num)\n",
    "accuracy_NB_bow = NB_clf_bow.score(bag_of_words_test_num, test_data['label'])\n",
    "precision_NB_bow = precision_score(test_data['label'], y_pred_NB_bow, average = 'macro')\n",
    "recall_NB_bow = recall_score(test_data['label'], y_pred_NB_bow, pos_label = 'spam')\n",
    "f1_NB_bow = f1_score(test_data['label'], y_pred_NB_bow, pos_label = 'spam')\n",
    "accuracy_NB_bow, precision_NB_bow, recall_NB_bow, f1_NB_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831bab5",
   "metadata": {},
   "source": [
    "##### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f2374680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7010594196222939,\n",
       " 0.6693778591843416,\n",
       " 0.6616989567809239,\n",
       " 0.5777488614183475)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_clf_sim = MultinomialNB().fit(similarity_train_num, train_data['label'])\n",
    "y_pred_NB_sim = NB_clf_sim.predict(similarity_test_num)\n",
    "accuracy_NB_sim = NB_clf_sim.score(similarity_test_num, test_data['label'])\n",
    "precision_NB_sim = precision_score(test_data['label'], y_pred_NB_sim, average = 'macro')\n",
    "recall_NB_sim = recall_score(test_data['label'], y_pred_NB_sim, pos_label = 'spam')\n",
    "f1_NB_sim = f1_score(test_data['label'], y_pred_NB_sim, pos_label = 'spam')\n",
    "accuracy_NB_sim, precision_NB_sim, recall_NB_sim, f1_NB_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b851c",
   "metadata": {},
   "source": [
    "## 3-5. Neural Network (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "abd8c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ada51",
   "metadata": {},
   "source": [
    "#### Cross-Validation (Train_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a9fda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_clf = MLPClassifier(solver = 'adam', activation = 'logistic').fit(features_train_num, train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "170d9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_scores = cross_val_score(NN_clf, features_train_num, train_data['label'], cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1566d87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666668"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(NN_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "72d117d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0064235215909678"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.96 * sqrt((np.mean(NN_scores) * (1 - np.mean(NN_scores))) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d248c1",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a5e3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_NN = NN_clf.predict(features_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff39935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_NN = NN_clf.score(features_test_num, test_data['label'])\n",
    "precision_NN = precision_score(test_data['label'], y_pred_NN, average = 'macro')\n",
    "recall_NN = recall_score(test_data['label'], y_pred_NN, pos_label = 'spam')\n",
    "f1_NN = f1_score(test_data['label'], y_pred_NN, pos_label = 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e166c818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9649930907415938, 0.9597156209129478, 0.940387481371088, 0.9431988041853514)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_NN, precision_NN, recall_NN, f1_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b34abfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_NN_interval = 1.96 * sqrt((accuracy_NN * (1 - accuracy_NN)) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10463ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007731524669671372"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_NN_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bb286",
   "metadata": {},
   "source": [
    "#### Contribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6da00b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Type</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>sj_new</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21773</th>\n",
       "      <td>ct_ken</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15909</th>\n",
       "      <td>ct_employee</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>ct_2004</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28424</th>\n",
       "      <td>ct_question</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16030</th>\n",
       "      <td>ct_enron</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13710</th>\n",
       "      <td>ct_daren</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22582</th>\n",
       "      <td>ct_let</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27317</th>\n",
       "      <td>ct_pm</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>sj_picture</td>\n",
       "      <td>bag of words</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature          Type  Importance\n",
       "2028        sj_new  bag of words    0.000255\n",
       "21773       ct_ken  bag of words    0.000235\n",
       "15909  ct_employee  bag of words    0.000231\n",
       "4028       ct_2004  bag of words    0.000230\n",
       "28424  ct_question  bag of words    0.000223\n",
       "16030     ct_enron  bag of words    0.000222\n",
       "13710     ct_daren  bag of words    0.000216\n",
       "22582       ct_let  bag of words    0.000215\n",
       "27317        ct_pm  bag of words    0.000214\n",
       "2220    sj_picture  bag of words    0.000214"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_NN = pd.DataFrame()\n",
    "feature_NN_cal = [sum(np.abs(feature)) for feature in NN_clf.coefs_[0]]\n",
    "feature_NN_cal_sum = sum(feature_NN_cal)\n",
    "feature_NN_cal = [num / feature_NN_cal_sum for num in feature_NN_cal]\n",
    "feature_importance_NN['Feature'] = features_name\n",
    "feature_importance_NN['Type'] = features_type\n",
    "feature_importance_NN['Importance'] = feature_NN_cal\n",
    "feature_importance_NN.sort_values(by = 'Importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb795be",
   "metadata": {},
   "source": [
    "##### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "454a31b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8249654537079687,\n",
       " 0.8144312387730974,\n",
       " 0.5842026825633383,\n",
       " 0.6735395189003437)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_clf_length = MLPClassifier(solver = 'adam', activation = 'logistic').fit(length_train_num, train_data['label'])\n",
    "y_pred_NN_length = NN_clf_length.predict(length_test_num)\n",
    "accuracy_NN_length = NN_clf_length.score(length_test_num, test_data['label'])\n",
    "precision_NN_length = precision_score(test_data['label'], y_pred_NN_length, average = 'macro')\n",
    "recall_NN_length = recall_score(test_data['label'], y_pred_NN_length, pos_label = 'spam')\n",
    "f1_NN_length = f1_score(test_data['label'], y_pred_NN_length, pos_label = 'spam')\n",
    "accuracy_NN_length, precision_NN_length, recall_NN_length, f1_NN_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23639c7f",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f7deca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.987563334868724, 0.9848643875287667, 0.9821162444113264, 0.9799256505576207)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_clf_bow = MLPClassifier(solver = 'adam', activation = 'logistic').fit(bag_of_words_train_num, train_data['label'])\n",
    "y_pred_NN_bow = NN_clf_bow.predict(bag_of_words_test_num)\n",
    "accuracy_NN_bow = NN_clf_bow.score(bag_of_words_test_num, test_data['label'])\n",
    "precision_NN_bow = precision_score(test_data['label'], y_pred_NN_bow, average = 'macro')\n",
    "recall_NN_bow = recall_score(test_data['label'], y_pred_NN_bow, pos_label = 'spam')\n",
    "f1_NN_bow = f1_score(test_data['label'], y_pred_NN_bow, pos_label = 'spam')\n",
    "accuracy_NN_bow, precision_NN_bow, recall_NN_bow, f1_NN_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c61647",
   "metadata": {},
   "source": [
    "##### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e190bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.833256563795486, 0.8179673557024951, 0.6304023845007451, 0.7003311258278146)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_clf_sim = MLPClassifier(solver = 'adam', activation = 'logistic').fit(similarity_train_num, train_data['label'])\n",
    "y_pred_NN_sim = NN_clf_sim.predict(similarity_test_num)\n",
    "accuracy_NN_sim = NN_clf_sim.score(similarity_test_num, test_data['label'])\n",
    "precision_NN_sim = precision_score(test_data['label'], y_pred_NN_sim, average = 'macro')\n",
    "recall_NN_sim = recall_score(test_data['label'], y_pred_NN_sim, pos_label = 'spam')\n",
    "f1_NN_sim = f1_score(test_data['label'], y_pred_NN_sim, pos_label = 'spam')\n",
    "accuracy_NN_sim, precision_NN_sim, recall_NN_sim, f1_NN_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6691a86",
   "metadata": {},
   "source": [
    "## 3-6. kNN\\*<br>\n",
    "\\* take a too long time to run (to find the best k) so I drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00aa9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, KFold\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier()\n",
    "# k_range = list(range(1, 11))\n",
    "# param_grid = dict(n_neighbors = k_range)\n",
    "# kf = KFold(n_splits = 10)\n",
    "# grid = GridSearchCV(knn, param_grid = param_grid, cv = kf)\n",
    "# grid.fit(features_train_num, train_data['label'])\n",
    "# cvKNN = grid.best_estimator_\n",
    "# y_pred_knn = cvKNN.predict(features_test_num)\n",
    "# accuracy_knn = cvKNN.score(features_test_num, test_data['label'])\n",
    "# precision_knn = precision_score(test_data['label'], y_pred_knn, average = 'macro')\n",
    "# accuracy_knn, precision_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b27f0",
   "metadata": {},
   "source": [
    "I end up do not choose kNN and random forest classifier because they all need a extra step to fine the best parameter (number of k and decision trees) to explore their best potential, but it will take a too long time to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70959804",
   "metadata": {},
   "source": [
    "## 3-7. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0c331d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "model_name = ['Dummy', 'LR', 'NB', 'DT', 'NN']\n",
    "predict_list = [y_pred_dummy, y_pred_LR, y_pred_NB, y_pred_DT, y_pred_NN]\n",
    "predictions['Real'] = test_data['label']\n",
    "for i in range(0, len(model_name)):\n",
    "    predictions[model_name[i]] = predict_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9eb069bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Dummy</th>\n",
       "      <th>LR</th>\n",
       "      <th>NB</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Real Dummy    LR    NB    DT    NN\n",
       "0   ham   ham   ham   ham   ham   ham\n",
       "1   ham   ham   ham   ham   ham   ham\n",
       "2  spam   ham  spam  spam  spam  spam\n",
       "3   ham   ham   ham   ham   ham   ham\n",
       "4   ham   ham   ham   ham   ham   ham\n",
       "5   ham   ham   ham   ham   ham   ham\n",
       "6  spam   ham   ham   ham  spam  spam\n",
       "7   ham   ham   ham   ham   ham   ham\n",
       "8  spam   ham   ham   ham  spam  spam\n",
       "9   ham   ham   ham   ham   ham   ham"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f812dc",
   "metadata": {},
   "source": [
    "# 4. Classifier Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2b6e5",
   "metadata": {},
   "source": [
    "## 4-1. Train_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a96fab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fd569e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_name = ['Dummy', 'LR', 'NB', 'DT', 'NN']\n",
    "ttest_list = [dummy_scores, LR_scores, NB_scores, DT_scores, NN_scores]\n",
    "ttest_mean = []\n",
    "ttest_result = []\n",
    "\n",
    "for i in range(0, len(ttest_list)):\n",
    "    ttest_mean.append(np.mean(ttest_list[i]))\n",
    "    t_result = []\n",
    "    for j in range(0, len(ttest_list)):\n",
    "        if not i == j:\n",
    "            t, p = ttest_rel(ttest_list[i], ttest_list[j])\n",
    "            if p < 0.05:\n",
    "                if np.mean(ttest_list[i]) > np.mean(ttest_list[j]):\n",
    "                    t_result.append(j)\n",
    "    ttest_result.append(t_result)\n",
    "\n",
    "ttest_df = pd.DataFrame()\n",
    "for n in range(0, len(ttest_name)):\n",
    "    ttest_df[ttest_name[n]] = [ttest_mean[n], ttest_result[n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9718caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = ttest_rel(ttest_list[1], ttest_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f4c0b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5ef89ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy</th>\n",
       "      <th>LR</th>\n",
       "      <th>NB</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.932667</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dummy     LR     NB         DT            NN\n",
       "0  0.724  0.857  0.851   0.932667      0.966667\n",
       "1     []    [0]    [0]  [0, 1, 2]  [0, 1, 2, 3]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e961c",
   "metadata": {},
   "source": [
    "## 4-2. Test_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "85d9256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "450bd02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006024237487153212"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_accuracy, p_accuracy = ttest_1samp([accuracy_LR, accuracy_NB, accuracy_DT, accuracy_NN], accuracy_dummy)\n",
    "p_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d3a71de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003513895130514171"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_precision, p_precision = ttest_1samp([precision_LR, precision_NB, precision_DT, precision_NN], precision_dummy)\n",
    "p_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5f1c5bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002149965699384519"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_recall, p_recall = ttest_1samp([recall_LR, recall_NB, recall_DT, recall_NN], recall_dummy)\n",
    "p_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f9a0885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007182723445649311"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_f1, p_f1 = ttest_1samp([f1_LR, f1_NB, f1_DT, f1_NN], f1_dummy)\n",
    "p_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02cbffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame()\n",
    "test_result['Dummy'] = [accuracy_dummy, precision_dummy, recall_dummy, f1_dummy]\n",
    "test_result['LR'] = [accuracy_LR, precision_LR, recall_LR, f1_LR]\n",
    "test_result['NB'] = [accuracy_NB, precision_NB, recall_NB, f1_NB]\n",
    "test_result['DT'] = [accuracy_DT, precision_DT, recall_DT, f1_DT]\n",
    "test_result['NN'] = [accuracy_NN, precision_NN, recall_NN, f1_NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ff4579f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dummy</th>\n",
       "      <th>LR</th>\n",
       "      <th>NB</th>\n",
       "      <th>DT</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.690926</td>\n",
       "      <td>0.842008</td>\n",
       "      <td>0.853063</td>\n",
       "      <td>0.925380</td>\n",
       "      <td>0.964993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345463</td>\n",
       "      <td>0.832889</td>\n",
       "      <td>0.852691</td>\n",
       "      <td>0.923512</td>\n",
       "      <td>0.959716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631893</td>\n",
       "      <td>0.634873</td>\n",
       "      <td>0.831595</td>\n",
       "      <td>0.940387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712007</td>\n",
       "      <td>0.727583</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.943199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dummy        LR        NB        DT        NN\n",
       "0  0.690926  0.842008  0.853063  0.925380  0.964993\n",
       "1  0.345463  0.832889  0.852691  0.923512  0.959716\n",
       "2  0.000000  0.631893  0.634873  0.831595  0.940387\n",
       "3  0.000000  0.712007  0.727583  0.873239  0.943199"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467fd63",
   "metadata": {},
   "source": [
    "## 4-3. Test v.s. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "edca1d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.226762225904206e-14"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dummy, p_dummy = ttest_1samp(dummy_scores, accuracy_dummy)\n",
    "p_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "13f7f901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10886247257610704"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_LR, p_LR = ttest_1samp(LR_scores, accuracy_LR)\n",
    "p_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4c6fbb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789478693202416"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_NB, p_NB = ttest_1samp(NB_scores, accuracy_NB)\n",
    "p_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "107ded23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11534852178155862"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_DT, p_DT = ttest_1samp(DT_scores, accuracy_DT)\n",
    "p_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5389de97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882717853350015"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_NN, p_NN = ttest_1samp(NN_scores, accuracy_NN)\n",
    "p_NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
